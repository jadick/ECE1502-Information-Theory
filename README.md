## ECE1502 - Information Theory - Fall 2023

### Lectures
| Lecture | Topic | Reference | Video|
| --- | --- | --- | --- |
| 1 | Introduction, source and channel coding, an axiomatic approach to defining a measure of "uncertainty" or entropy. | [Axiomatic Definition of Entropy](Reviews/axiomatic.pdf)  |  |
| 2 | Review of discrete probability, the weak law of large numbers. | [Discrete Probability Refresher](Reviews/Probability%20Review.pdf) |  |
| 3 | Entropy, joint entropy, conditional entropy, the asymptotic equipartition property (AEP)	 | C&T Sections 2.1, 2.2, 3.1	 |  |
| 4 | The AEP, typical sequences and properties of the typical set.	 | C&T Section 3.1	 |  |
| 5 | Source coding (variable-length and fixed-length codes) as an application of typicality.	 | C&T Section 3.2, Gallager Section 3.1	 |  |
| 6 | High probability sets, mutual information, relative entropy.	 | C&T Sections 3.3, 2.3	 |  |
| 7 | Information inequalities, Markov chains, the data-processing inequality.	 | C&T Sections 2.4, 2.5, 2.8	 |  |
| 8 | Data compression codes, prefix codes, Kraft's inequality.	 | C&T Sections 5.1, 5.2	 |  |
| 9 | McMillan's theorem, entropy as a lower bound on expected codeword length, Shannon codes.	 | C&T Sections 5.3, 5.4, 5.5, [generatingfunctionology](https://www2.math.upenn.edu/~wilf/DownldGF.html) |  |
| 10 | Approaching entropy by source extension, penalty for incorrect length assignment, Huffman codes.	 | C&T Sections 5.4, 5.6	 |  |
| 11 | Optimality of Huffman codes, dual tree codes, Tunstall codes.	 | C&T Section 5.8, Moser Sections 5.6, 5.7 (optional: [recent paper](https://dl.acm.org/doi/pdf/10.1145/3230653)) |  |
| 12 | Arithmetic coding, Jensen's inequality, the log sum inequality.	 | C&T Section 5.9, Moser Section 5.3, C&T Sections 2.6, 2.7 |  |
| 13 | Universal source coding:  Fitingof's scheme, Lempel-Ziv coding.	 | C&T Section 13.2, 13.4, 13.5 |  |
| 14 | Stochastic processes, stationarity, Markov chains, entropy rate, conditional entropy rate.	 | C&T Section 4.1, 4.2; also mentioned in the context of midterm question 5: [Fortune's Formula](http://www.fortunesformula.com/). by William Poundstone and  A Man for [All Markets](https://www.edwardothorp.com/books/a-man-for-all-markets/) by Edward O. Thorp. |  |
| 15 | Entropy rates for Markov chains in steady state, random walks on weighted graphs.	 | C&T Section 4.3	
 |  |
| 16 | Discrete memoryless channels, information theoretic capacity.	 | C&T Section 7.1	
 |  |
| 17 | Comments on midterm; detour on rates in source and channel coding; back to the course: weakly symmetric channels.	 | C&T Section 7.2	
 |  |
| 18 | Computing channel capacity via the Blahut-Arimoto algorithm.	 | C&T Section 10.8	
 |  |
| 19 | Channel Coding Theorem (Part 1)	 | C&T Sections 7.4, 7.5, 7.6	
 |  |
| 20 | Channel Coding Theorem (Part 2)	 | C&T Sections 7.6, 7.7	
 |  |
| 21 | Channel Coding Theorem (Part 3)	 | C&T Sections 7.7, 7.9	
 |  |
| 22 | Linear block codes; polar codes.	 | C&T Section 7.11, Moser Chapter 14.	
 |  |
| 23 | Differential entropy	 | C&T Sections 8.1, 8.2 8.3 8.4, 8.5, 8.6	
 |  |
| 24 | Capacity of the additive white Gaussian noise channel.	 | C&T Sections 9.1, 9.4	
 |  |


1. Introduction, source and channel coding, an axiomatic approach to defining a measure of "uncertainty" or entropy.
2. Review of discrete probability, the weak law of large numbers.
3. Entropy, joint entropy, conditional entropy, the asymptotic equipartition property (AEP)
4. The AEP, typical sequences and properties of the typical set.
5. Source coding (variable-length and fixed-length codes) as an application of typicality.
6.
7.
8.
9.
10.